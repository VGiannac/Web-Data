{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3beeab-8e1b-48db-94d8-fc4a4f78818e",
   "metadata": {},
   "source": [
    "<h1><center> Web Data Wizardry with Python  </center></h1>\n",
    "\n",
    "\n",
    "# Vinicius Rodrigues Giannaccini \n",
    "# 11/21/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19251f1-0066-43b6-83c7-ff2abd152611",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#zero-bullet)\n",
    "* [XML Parsing](#first-bullet)\n",
    "* [Using an API](#second-bullet)\n",
    "* [Analyze dataset](#third-bullet)\n",
    "* [References & Appendix](#fourth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6ef37-e039-40a2-8769-98bff1717e82",
   "metadata": {},
   "source": [
    "## [Introduction](#zero-bullet)\n",
    "\n",
    "Web data extraction and modification are critical tasks in the fields of data science and analytics. With an emphasis on HTML, XML, JSON parsing, and API interaction, Project 2 sets out to explore the possibilities of utilizing Python for web data collecting. This project explores three different tasks that combine technical expertise and creative exploration:\n",
    "\n",
    "- **XML Parsing**: We extract useful information from XML sitemaps and use Python to traverse the HTML structure of webpages, transforming unprocessed data into clean Pandas DataFrames.\n",
    "\n",
    "- **Using an API**: We take advantage of a free API's capabilities and use Python's power to gather data and convert it into a DataFrame for meaningful analysis. The dataset's and API's uniqueness enhances the depth of our investigation.\n",
    "\n",
    "- **Web Scraping**: Using BeautifulSoup to its full potential, we browse a webpage and scrape a distinct dataset. Our data toolbox gains additional intricacy and variety from this web scraping endeavor. \n",
    "\n",
    "In order to bring this journey to a close, we analyze the data collected in order to find trends and insights. Because each piece is contained within a modular Python package, code and explanation are always seamlessly integrated.\n",
    "    Come along as we combine technical expertise with analytical savvy to unleash the power of online data.\n",
    "\n",
    "Let's start this data-driven journey now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1074c1d8-8cbc-438a-b207-45a458942a0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'web_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filterwarnings \n\u001b[0;32m     10\u001b[0m filterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweb_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxml_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XmlParser\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweb_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApiHandler\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweb_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb_scraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebScraper\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'web_data'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for web data processing\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# ... (other libraries related to web data)\n",
    "\n",
    "# Disable warnings if needed\n",
    "from warnings import filterwarnings \n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "from web_data.xml_parser import XmlParser\n",
    "from web_data.api_handler import ApiHandler\n",
    "from web_data.web_scraper import WebScraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91849065-0519-4432-b803-d55faa097300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
